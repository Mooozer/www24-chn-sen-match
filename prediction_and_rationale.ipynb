{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import zhon\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import string\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import Levenshtein\n",
    "import scipy.stats as ss\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from LAC import LAC\n",
    "from matplotlib.pyplot import figure\n",
    "from zhon.hanzi import punctuation as CHN_punctuation\n",
    "\n",
    "TEST_DATA_PATH = \"./data/sim_interpretation_B.txt\"\n",
    "LIME_SCORE_PATH = './lime_scores/output.npy'\n",
    "SHAP_SCORE_PATH = './shap_scores/output.npy'\n",
    "IG_SCORE_PATH = './ig_scores/output.npy'\n",
    "\n",
    "MACBERT_LOGITS_PATH = \"./macbert_large_without_EF/predictionB/logits_B_test.npy\"\n",
    "MACBERT_EF_LOGITS_PATH = \"./macbert_large_with_EF/predictionB/logits_B_test.npy\"\n",
    "\n",
    "ROBERTA_LOGITS_PATH = \"./roberta_large_without_EF/predictionB/logits_B_test.npy\"\n",
    "ROBERTA_EF_LOGITS_PATH = \"./roberta_large_with_EF/predictionB/logits_B_test.npy\"\n",
    "\n",
    "BERT_CHN_LOGITS_PATH = \"./bert_base_chinese_without_EF/predictionB/logits_B_test.npy\"\n",
    "BERT_CHN_EF_LOGITS_PATH = \"./bert_base_chinese_with_EF/predictionB/logits_B_test.npy\"\n",
    "\n",
    "BERT_MULLING_LOGITS_PATH = \"./bert_base_multilingual_without_EF/predictionB/logits_B_test.npy\"\n",
    "BERT_MULLING_EF_LOGITS_PATH = \"./bert_base_multilingual_with_EF/predictionB/logits_B_test.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff250300",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENG_punctuation =  string.punctuation\n",
    "P_LIST = list(ENG_punctuation) + list(CHN_punctuation) \n",
    "stopwordsFile = open(\"./baidu_stopwords.txt\", \"r\")\n",
    "baidu_stopwords = stopwordsFile.read()  \n",
    "STOPWORDS= baidu_stopwords.split('\\n')\n",
    "RM_TOKENS = STOPWORDS + P_LIST\n",
    "lac = LAC(mode='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "#function for timing \n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def Levenshtein_similarity(string1, string2):\n",
    "    '''\n",
    "    output: scalar, Levenshtein similarity of string1 & string2\n",
    "    '''\n",
    "    Levenshtein_ratio = Levenshtein.ratio(string1, string2)\n",
    "    return(Levenshtein_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-strip",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCQMC_testB_dic = {'id':[], 'query':[], 'title':[], 'text_q_seg':[],'text_t_seg':[] } \n",
    "with open(TEST_DATA_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        line_dic = json.loads(line)\n",
    "        for k in line_dic.keys():\n",
    "            LCQMC_testB_dic[k].append(line_dic[k])\n",
    "\n",
    "LCQMC_testB = pd.DataFrame.from_dict(LCQMC_testB_dic)\n",
    "LCQMC_testB['sentence'] = LCQMC_testB['query'] +\"[SEP]\" + LCQMC_testB['title']\n",
    "LCQMC_testB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-equation",
   "metadata": {},
   "source": [
    "### Bayes Iteration Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd549f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_DATA_PATH  = './GOLD_DATA_PATH/Data.txt'\n",
    "with open(GOLD_DATA_PATH,'r') as file:\n",
    "    lines = file.readlines()\n",
    "data_B = []\n",
    "for line in lines:\n",
    "    data_B.append(json.loads(line))\n",
    "data_B = pd.DataFrame(data_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b6d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "     return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "def refine_sen(sen):\n",
    "    refine_sen = ''.join([i for i in list(sen) if i not in RM_TOKENS])\n",
    "    return refine_sen\n",
    "\n",
    "def update_prob(p, pie):\n",
    "    p = p*pie/(p*pie+(1-p)*(1-pie))\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_iteration_prediction(logits, infomation, num_iter, print_log=True):\n",
    "    ''' \n",
    "    Use Bayes iteration prediction algorithm to obtain the predicted labels\n",
    "    \n",
    "    logits: binary classification logits\n",
    "    infomation: data for providing infomation, dataset that has [['id','text_q','text_t','sent_label']], in which 'label' is only for accuracy calculating \n",
    "    num_iter: number of iteration\n",
    "    print_log: whether to print the log \n",
    "    '''\n",
    "    prob = softmax(logits.transpose()).transpose()\n",
    "    pred_l = np.argmax(logits ,axis=1)\n",
    "    if print_log:\n",
    "        print('pred_iter0', sum(np.array(data_B['sent_label']) == pred_l)/len(pred_l))\n",
    "    BayesDataB = infomation.copy()[['sent_id']]\n",
    "\n",
    "    fined_sq,fined_st = [],[]\n",
    "    for i in range(len(infomation)):\n",
    "        fined_sq.append(infomation['text_q'][i])\n",
    "        fined_st.append(infomation['text_t'][i]) \n",
    "    BayesDataB['pred_iter0'] = pred_l\n",
    "    BayesDataB['prob_iter0'] = prob[:,1]\n",
    "    BayesDataB['fined_sq'] = [refine_sen(s) for s in fined_sq]\n",
    "    BayesDataB['fined_st'] = [refine_sen(s) for s in fined_st]\n",
    "\n",
    "    BayesQ = {}\n",
    "    for i in range(len(BayesDataB)):\n",
    "        if BayesDataB['fined_sq'][i] not in BayesQ:\n",
    "            BayesQ[BayesDataB['fined_sq'][i]] = [1, BayesDataB['prob_iter0'][i], BayesDataB['prob_iter0'][i]/1]\n",
    "        else: \n",
    "            BayesQ[BayesDataB['fined_sq'][i]][0]+=1\n",
    "            BayesQ[BayesDataB['fined_sq'][i]][1]+=BayesDataB['prob_iter0'][i]\n",
    "            BayesQ[BayesDataB['fined_sq'][i]][2]= BayesQ[BayesDataB['fined_sq'][i]][1]/BayesQ[BayesDataB['fined_sq'][i]][0]\n",
    "        if BayesDataB['fined_st'][i] not in BayesQ:\n",
    "            BayesQ[BayesDataB['fined_st'][i]] = [1, BayesDataB['prob_iter0'][i], BayesDataB['prob_iter0'][i]/1]\n",
    "        else: \n",
    "            BayesQ[BayesDataB['fined_st'][i]][0]+=1\n",
    "            BayesQ[BayesDataB['fined_st'][i]][1]+=BayesDataB['prob_iter0'][i]\n",
    "            BayesQ[BayesDataB['fined_st'][i]][2]= BayesQ[BayesDataB['fined_st'][i]][1]/BayesQ[BayesDataB['fined_st'][i]][0]\n",
    "\n",
    "    acc_bayes_records = [sum(np.array(data_B['sent_label']) == pred_l)/len(pred_l)] \n",
    "    for b in range(num_iter):\n",
    "        pred_next = []\n",
    "        prob_next = []\n",
    "        for i in range(len(BayesDataB)):\n",
    "            pie1 = max(BayesQ[BayesDataB['fined_sq'][i]][2], BayesQ[BayesDataB['fined_st'][i]][2])\n",
    "            p1 = BayesDataB['prob_iter'+str(b)][i]\n",
    "            p1_update = update_prob(p1, pie1)\n",
    "            pred_next.append(int((p1_update>0.5)==True))\n",
    "            prob_next.append(p1_update)\n",
    "        BayesDataB['pred_iter'+str(b+1)] = pred_next\n",
    "        BayesDataB['prob_iter'+str(b+1)] = prob_next\n",
    "        acc_bayes_records.append(sum(np.array(data_B['sent_label']) == pred_next)/len(pred_next))\n",
    "        if print_log:\n",
    "            print('pred_iter'+str(b+1),sum(np.array(data_B['sent_label']) == pred_next)/len(pred_next))\n",
    "        \n",
    "    return {'final_prediction': pred_next,  'acc_records': acc_bayes_records}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437e1f4",
   "metadata": {},
   "source": [
    "### Multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa79b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "macbert_large_logits = np.load(MACBERT_LOGITS_PATH, allow_pickle=True)\n",
    "result = Bayes_iteration_prediction(macbert_large_logits, data_B, 14, print_log=True)\n",
    "final_prediction_macbert, acc_records = result['final_prediction'], np.array(result['acc_records'])*100\n",
    "np.save('./macbert_large_without_EF/predictionB/bayes_prediction_label.npy', np.array(final_prediction_macbert))\n",
    "\n",
    "macbert_large_with_EF_logits = np.load(MACBERT_EF_LOGITS_PATH, allow_pickle=True)\n",
    "result_EF = Bayes_iteration_prediction(macbert_large_with_EF_logits, data_B, 14, print_log=True)\n",
    "final_prediction_macbert_EF, acc_records_EF = result_EF['final_prediction'], np.array(result_EF['acc_records'])*100\n",
    "np.save('./macbert_large_with_EF/predictionB/bayes_prediction_label.npy', np.array(final_prediction_macbert_EF))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(5, 3.5),dpi=100)\n",
    "for i in range(len(acc_records)):\n",
    "    plt.axhline(y=acc_records[i], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records[0], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records[i], xmin=0, xmax=i, color='grey', linestyle='--')\n",
    "plt.plot(acc_records, '-^', color = 'orange', label = 'MacBERT-BIP')\n",
    "\n",
    "for i in range(len(acc_records_EF)):\n",
    "    plt.axhline(y=acc_records_EF[i], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records_EF[0], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records_EF[i], xmin=0, xmax=i, color='grey', linestyle='--')\n",
    "plt.plot(acc_records_EF, '-^', color = 'firebrick', label = 'MacBERT-EF-BIP')\n",
    "plt.xlim((-1.5,15))\n",
    "plt.xlabel('Bayes iteration',fontsize=15)\n",
    "plt.ylabel('Accuracy',fontsize=15)\n",
    "plt.legend(loc = 'lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_large_logits = np.load(ROBERTA_LOGITS_PATH, allow_pickle=True)\n",
    "result = Bayes_iteration_prediction(roberta_large_logits, data_B, 14)\n",
    "final_prediction_roberta, acc_records = result['final_prediction'], np.array(result['acc_records'])*100\n",
    "np.save('./roberta_large_without_EF/predictionB/bayes_prediction_label.npy', np.array(final_prediction_roberta))\n",
    "\n",
    "roberta_large_EF_logits = np.load(ROBERTA_EF_LOGITS_PATH, allow_pickle=True)\n",
    "result_EF = Bayes_iteration_prediction(roberta_large_EF_logits, data_B, 14)\n",
    "final_prediction_roberta_EF, acc_records_EF = result_EF['final_prediction'], np.array(result_EF['acc_records'])*100\n",
    "np.save('./roberta_large_with_EF/predictionB/bayes_prediction_label.npy', np.array(final_prediction_roberta_EF))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(5, 3.5),dpi=100)\n",
    "for i in range(len(acc_records)):\n",
    "    plt.axhline(y=acc_records[i], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records[0], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records[i], xmin=0, xmax=i, color='grey', linestyle='--')\n",
    "plt.plot(acc_records, '-^', color = 'orange', label = 'RoBERTa-BIP')\n",
    "\n",
    "for i in range(len(acc_records_EF)):\n",
    "    plt.axhline(y=acc_records_EF[i], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records_EF[0], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records_EF[i], xmin=0, xmax=i, color='grey', linestyle='--')\n",
    "plt.plot(acc_records_EF, '-^', color = 'firebrick', label = 'RoBERTa-EF-BIP')\n",
    "\n",
    "plt.xlim((-1.5,15))\n",
    "plt.ylim((86, 89))\n",
    "plt.xlabel('Bayes iteration',fontsize=15)\n",
    "plt.ylabel('Accuracy',fontsize=15)\n",
    "plt.legend(loc = 'lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base_chinese_logits = np.load(BERT_CHN_LOGITS_PATH, allow_pickle=True)\n",
    "result = Bayes_iteration_prediction(bert_base_chinese_logits, data_B, 14)\n",
    "final_prediction_bert_base_chinese, acc_records = result['final_prediction'], np.array(result['acc_records'])*100\n",
    "np.save('./bert_base_chinese_without_EF/predictionB/bayes_prediction_label.npy', np.array(final_prediction_bert_base_chinese))\n",
    "\n",
    "bert_base_chinese_EF_logits = np.load(BERT_CHN_EF_LOGITS_PATH, allow_pickle=True)\n",
    "result_EF = Bayes_iteration_prediction(bert_base_chinese_EF_logits, data_B, 14)\n",
    "final_prediction_bert_base_chinese_EF, acc_records_EF = result_EF['final_prediction'], np.array(result_EF['acc_records'])*100\n",
    "np.save('./bert_base_chinese_with_EF/predictionB/bayes_prediction_label.npy', np.array(final_prediction_bert_base_chinese_EF))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(5, 3.5),dpi=100)\n",
    "for i in range(len(acc_records)):\n",
    "    plt.axhline(y=acc_records[i], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records[0], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records[i], xmin=0, xmax=i, color='grey', linestyle='--')\n",
    "plt.plot(acc_records, '-^', color = 'orange', label = 'bert-chinese-BIP')\n",
    "\n",
    "for i in range(len(acc_records_EF)):\n",
    "    plt.axhline(y=acc_records_EF[i], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records_EF[0], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records_EF[i], xmin=0, xmax=i, color='grey', linestyle='--')\n",
    "plt.plot(acc_records_EF, '-^', color = 'firebrick', label = 'bert-chinese-EF-BIP')\n",
    "\n",
    "plt.xlim((-1.5,15))\n",
    "plt.xlabel('Bayes iteration',fontsize=15)\n",
    "plt.ylabel('Accuracy',fontsize=15)\n",
    "plt.legend(loc = 'lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base_multilang_logits = np.load(BERT_MULLING_LOGITS_PATH, allow_pickle=True)\n",
    "result = Bayes_iteration_prediction(bert_base_multilang_logits, data_B, 14)\n",
    "final_prediction_bert_base_multilang, acc_records = result['final_prediction'], np.array(result['acc_records'])*100\n",
    "np.save('./bert_base_multilingual_without_EF/predictionB/bayes_prediction_label.npy', np.array(final_prediction_bert_base_multilang))\n",
    "\n",
    "bert_base_multilang_EF_logits = np.load(BERT_MULLING_EF_LOGITS_PATH, allow_pickle=True)\n",
    "result_EF = Bayes_iteration_prediction(bert_base_multilang_EF_logits, data_B, 14)\n",
    "final_prediction_bert_base_multilang_EF, acc_records_EF = result_EF['final_prediction'], np.array(result_EF['acc_records'])*100\n",
    "np.save('./bert_base_multilingual_with_EF/predictionB/bayes_prediction_label.npy', np.array(final_prediction_bert_base_multilang_EF))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(5, 3.5),dpi=100)\n",
    "for i in range(len(acc_records)):\n",
    "    plt.axhline(y=acc_records[i], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records[0], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records[i], xmin=0, xmax=i, color='grey', linestyle='--')\n",
    "plt.plot(acc_records, '-^', color = 'orange', label = 'bert-multilingual-BIP')\n",
    "\n",
    "for i in range(len(acc_records_EF)):\n",
    "    plt.axhline(y=acc_records_EF[i], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records_EF[0], xmin=0, xmax=i, color='lightgrey', linestyle='--')\n",
    "plt.axhline(y=acc_records_EF[i], xmin=0, xmax=i, color='grey', linestyle='--')\n",
    "plt.plot(acc_records_EF, '-^', color = 'firebrick', label = 'bert-multilingual-EF-BIP')\n",
    "\n",
    "plt.xlim((-1.5,15))\n",
    "plt.xlabel('Bayes iteration',fontsize=15)\n",
    "plt.ylabel('Accuracy',fontsize=15)\n",
    "plt.legend(loc = 'lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe8d33",
   "metadata": {},
   "source": [
    "### Rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder(score, token_index, mode='abs'):\n",
    "    '''\n",
    "    input\n",
    "        score: list\n",
    "        token_index: list\n",
    "        mode: ['abs' or 'sequence' or 'token_seq']\n",
    "    output\n",
    "        reordered score and token lists based on corresponding score\n",
    "    '''\n",
    "    assert len(score) == len(token_index)\n",
    "    output = {}\n",
    "    if mode =='abs':\n",
    "        abs_score = [abs(i) for i in score]\n",
    "        abs_dic = dict(zip(token_index, abs_score))\n",
    "        sorted_abs_dic = dict(sorted(abs_dic.items(), key=lambda item: item[1], reverse=True))\n",
    "        output['sorted_token_index'] = list(sorted_abs_dic.keys())\n",
    "        output['sorted_score'] = list(sorted_abs_dic.values())\n",
    "    \n",
    "    if mode =='sequence':\n",
    "        score_dic = dict(zip(token_index, score))\n",
    "        sorted_score_dic = dict(sorted(score_dic.items(), key=lambda item: item[1], reverse=True))\n",
    "        output['sorted_token_index'] = list(sorted_score_dic.keys())\n",
    "        output['sorted_score'] = list(sorted_score_dic.values())\n",
    "\n",
    "    if mode =='token_seq':\n",
    "        abs_dic = dict(zip(token_index, score))\n",
    "        sorted_abs_dic = dict(sorted(abs_dic.items(), key=lambda item: item[0], reverse=False))\n",
    "        output['sorted_token_index'] = list(sorted_abs_dic.keys())\n",
    "        output['sorted_score'] = list(sorted_abs_dic.values())\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-subcommittee",
   "metadata": {},
   "source": [
    "#### LAC scores and Lime scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_LAC_expand(LAC_result):\n",
    "    '''\n",
    "    input: single LAC_result: list of list: [LAC_token, LAC_ner, LAC_imp]\n",
    "    output: expanded LAC_result: list of list: [LAC_token, LAC_ner, LAC_imp, num]\n",
    "    '''\n",
    "    number = [len(list(i)) for i in LAC_result[0]]\n",
    "    expand_LAC_token = sum([list(i) for i in LAC_result[0]],[])\n",
    "    expand_LAC_enr = sum([[LAC_result[1][i]]*number[i] for i in range(len(LAC_result[1]))],[])\n",
    "    expand_LAC_imp =  sum([[LAC_result[2][i]]*number[i] for i in range(len(LAC_result[2]))],[])\n",
    "    \n",
    "    return [expand_LAC_token, expand_LAC_enr, expand_LAC_imp, number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-africa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lac = LAC(mode='rank')\n",
    "dual_ranking_rationale = np.load(LIME_SCORE_PATH, allow_pickle=True).item()\n",
    "shap_scores = np.load(SHAP_SCORE_PATH, allow_pickle=True).item()\n",
    "ig_scores = np.load(IG_SCORE_PATH, allow_pickle=True).item()\n",
    "\n",
    "dual_ranking_rationale['token'] = []\n",
    "dual_ranking_rationale['query'] = []\n",
    "dual_ranking_rationale['title'] = []\n",
    "dual_ranking_rationale['ner'] = []\n",
    "dual_ranking_rationale['lac'] = []\n",
    "dual_ranking_rationale['num'] = []\n",
    "dual_ranking_rationale['piece'] = []\n",
    "dual_ranking_rationale['laclime'] = [] #LAC-wise mean of token lime score \n",
    "dual_ranking_rationale['laclime_rank'] = [] \n",
    "dual_ranking_rationale['ori_laclime_rank'] = []\n",
    "dual_ranking_rationale['shap_score'] = shap_scores['rationale_score']\n",
    "dual_ranking_rationale['ig_score'] = ig_scores['rationale_score']\n",
    "\n",
    "for i in range(len(dual_ranking_rationale['id'])): \n",
    "    \n",
    "    q = reorder(dual_ranking_rationale['rationale_score'][i][0], dual_ranking_rationale['rationale'][i][0], mode='token_seq')\n",
    "    t = reorder(dual_ranking_rationale['rationale_score'][i][1], dual_ranking_rationale['rationale'][i][1], mode='token_seq')\n",
    "    \n",
    "    dual_ranking_rationale['rationale'][i] = [q['sorted_token_index'], t['sorted_token_index']]\n",
    "    dual_ranking_rationale['rationale_score'][i] = [q['sorted_score'], t['sorted_score']]    \n",
    "\n",
    "    dual_ranking_rationale['token'].append([LCQMC_testB['text_q_seg'][i], LCQMC_testB['text_t_seg'][i]])\n",
    "    dual_ranking_rationale['query'].append(LCQMC_testB['query'][i])\n",
    "    dual_ranking_rationale['title'].append(LCQMC_testB['title'][i])\n",
    "    LAC_result_q, LAC_result_t = lac.run(LCQMC_testB['query'][i]), lac.run(LCQMC_testB['title'][i])\n",
    "    dual_ranking_rationale['piece'].append([LAC_result_q[0],LAC_result_t[0]])\n",
    "    LAC_result_q, LAC_result_t = single_LAC_expand(LAC_result_q) ,single_LAC_expand(LAC_result_t) \n",
    "\n",
    "    LAC_token_q, LAC_token_t = LAC_result_q[0], LAC_result_t[0]\n",
    "    LAC_ner_q, LAC_ner_t = LAC_result_q[1], LAC_result_t[1]\n",
    "    LAC_imp_q, LAC_imp_t = LAC_result_q[2], LAC_result_t[2]\n",
    "    LAC_num_q, LAC_num_t = LAC_result_q[3], LAC_result_t[3]\n",
    "    \n",
    "    \n",
    "    if LCQMC_testB['text_q_seg'][i] == LAC_token_q and LCQMC_testB['text_t_seg'][i] == LAC_token_t:\n",
    "        dual_ranking_rationale['ner'].append([LAC_ner_q, LAC_ner_t])\n",
    "        dual_ranking_rationale['lac'].append([LAC_imp_q, LAC_imp_t])\n",
    "        dual_ranking_rationale['num'].append([LAC_num_q, LAC_num_t])\n",
    "        #add LIME scores based on LAC segmentation (mean)\n",
    "        c_sum_q, c_sum_t = [0] + list(np.cumsum(LAC_num_q)), [0] + list(np.cumsum(LAC_num_t)) \n",
    "        laclime_q = sum([[np.mean(q['sorted_score'][c_sum_q[k]:c_sum_q[k+1]])]*LAC_num_q[k] for k in range(len(LAC_num_q))],[])\n",
    "        laclime_t = sum([[np.mean(t['sorted_score'][c_sum_t[k]:c_sum_t[k+1]])]*LAC_num_t[k] for k in range(len(LAC_num_t))],[])\n",
    "        dual_ranking_rationale['laclime'].append([laclime_q, laclime_t])\n",
    "        dual_ranking_rationale['laclime_rank'].append([ list(ss.rankdata([abs(x) for x in laclime_q])), \n",
    "                                                  list(ss.rankdata([abs(x) for x in laclime_t])) ])\n",
    "        dual_ranking_rationale['ori_laclime_rank'].append([ list(ss.rankdata([x for x in laclime_q])), \n",
    "                                                  list(ss.rankdata([x for x in laclime_t])) ])\n",
    "        \n",
    "    else:\n",
    "        j = 0\n",
    "        while j < len(LCQMC_testB['text_q_seg'][i]):\n",
    "            if LCQMC_testB['text_q_seg'][i][j] == LAC_result_q[0][j]:\n",
    "                j+=1\n",
    "            else:\n",
    "                LAC_result_q[0][j] = LAC_result_q[0][j]+LAC_result_q[0][j+1]\n",
    "                del LAC_result_q[0][j+1]\n",
    "                LAC_result_q[1][j] = LAC_result_q[1][j]  #use the first ner\n",
    "                del LAC_result_q[1][j+1]\n",
    "                LAC_result_q[2][j] = int((LAC_result_q[2][j]+LAC_result_q[2][j+1])/2)\n",
    "                del LAC_result_q[2][j+1]\n",
    "                need_reduce_idx = [j - k < 0 for k in np.cumsum(LAC_result_q[3])].index(True)\n",
    "                LAC_result_q[3][need_reduce_idx] = LAC_result_q[3][need_reduce_idx]-1\n",
    "            \n",
    "        j = 0\n",
    "        while j < len(LCQMC_testB['text_t_seg'][i]):\n",
    "            if LCQMC_testB['text_t_seg'][i][j] == LAC_result_t[0][j]:\n",
    "                j+=1\n",
    "            else:\n",
    "                LAC_result_t[0][j] = LAC_result_t[0][j]+LAC_result_t[0][j+1]\n",
    "                del LAC_result_t[0][j+1]\n",
    "                LAC_result_t[1][j] = LAC_result_t[1][j] #use the first ner\n",
    "                del LAC_result_t[1][j+1]\n",
    "                LAC_result_t[2][j] = int((LAC_result_t[2][j]+LAC_result_t[2][j+1])/2)\n",
    "                del LAC_result_t[2][j+1]\n",
    "                need_reduce_idx = [j - k < 0 for k in np.cumsum(LAC_result_t[3])].index(True)\n",
    "                LAC_result_t[3][need_reduce_idx] = LAC_result_t[3][need_reduce_idx]-1\n",
    "                \n",
    "        LAC_token_q, LAC_token_t = LAC_result_q[0], LAC_result_t[0]\n",
    "        LAC_ner_q, LAC_ner_t = LAC_result_q[1], LAC_result_t[1]\n",
    "        LAC_imp_q, LAC_imp_t = LAC_result_q[2], LAC_result_t[2]\n",
    "        LAC_num_q, LAC_num_t = LAC_result_q[3], LAC_result_t[3]\n",
    "        dual_ranking_rationale['ner'].append([LAC_ner_q, LAC_ner_t])\n",
    "        dual_ranking_rationale['lac'].append([LAC_imp_q, LAC_imp_t])\n",
    "        dual_ranking_rationale['num'].append([LAC_num_q, LAC_num_t])\n",
    "\n",
    "        #add LIME scores based on LAC segmentation (mean)\n",
    "        c_sum_q, c_sum_t = [0] + list(np.cumsum(LAC_num_q)), [0] + list(np.cumsum(LAC_num_t)) \n",
    "        laclime_q = sum([[np.mean(q['sorted_score'][c_sum_q[k]:c_sum_q[k+1]])]*LAC_num_q[k] for k in range(len(LAC_num_q))],[])\n",
    "        laclime_t = sum([[np.mean(t['sorted_score'][c_sum_t[k]:c_sum_t[k+1]])]*LAC_num_t[k] for k in range(len(LAC_num_t))],[])\n",
    "        dual_ranking_rationale['laclime'].append([laclime_q, laclime_t])\n",
    "        dual_ranking_rationale['laclime_rank'].append([ list(ss.rankdata([abs(x) for x in laclime_q])), \n",
    "                                                  list(ss.rankdata([abs(x) for x in laclime_t])) ])\n",
    "        dual_ranking_rationale['ori_laclime_rank'].append([ list(ss.rankdata([x for x in laclime_q])), \n",
    "                                                  list(ss.rankdata([x for x in laclime_t])) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-average",
   "metadata": {},
   "source": [
    "###  lexical category scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_a,ner_b,ner_c,ner_d = [],[],[],[]\n",
    "for i in range(len(dual_ranking_rationale['lac'] )):\n",
    "    for j in range(len(dual_ranking_rationale['lac'][i][0])):\n",
    "        if dual_ranking_rationale['lac'][i][0][j] == 3:\n",
    "            ner_a.append(dual_ranking_rationale['ner'][i][0][j])\n",
    "        if dual_ranking_rationale['lac'][i][0][j] == 2:\n",
    "            ner_b.append(dual_ranking_rationale['ner'][i][0][j])\n",
    "        if dual_ranking_rationale['lac'][i][0][j] == 1:\n",
    "            ner_c.append(dual_ranking_rationale['ner'][i][0][j])\n",
    "        if dual_ranking_rationale['lac'][i][0][j] == 0:\n",
    "            ner_d.append(dual_ranking_rationale['ner'][i][0][j])\n",
    "    for j in range(len(dual_ranking_rationale['lac'][i][1])):\n",
    "        if dual_ranking_rationale['lac'][i][1][j] == 3:\n",
    "            ner_a.append(dual_ranking_rationale['ner'][i][1][j])\n",
    "        if dual_ranking_rationale['lac'][i][1][j] == 2:\n",
    "            ner_b.append(dual_ranking_rationale['ner'][i][1][j])\n",
    "        if dual_ranking_rationale['lac'][i][1][j] == 1:\n",
    "            ner_c.append(dual_ranking_rationale['ner'][i][1][j])\n",
    "        if dual_ranking_rationale['lac'][i][1][j] == 0:\n",
    "            ner_d.append(dual_ranking_rationale['ner'][i][1][j])\n",
    "\n",
    "aug_frequency = collections.Counter(ner_a + ner_a + ner_a + ner_a+ ner_b + ner_b +ner_b + ner_c + ner_c+ ner_d)\n",
    "ori_frequency = collections.Counter(ner_a + ner_b + ner_c + ner_d)\n",
    "fine_imp_dic = {k: round(2*aug_frequency[k]/ori_frequency[k])/2 for k in ori_frequency.keys()}   \n",
    "\n",
    "print(dict(sorted(fine_imp_dic.items(), key=lambda item: -item[1])))\n",
    "dual_ranking_rationale['lexicality'] = []\n",
    "for i in range(len(dual_ranking_rationale['id'])):\n",
    "    fine_imp_q = [fine_imp_dic[k] for k in dual_ranking_rationale['ner'][i][0]]\n",
    "    fine_imp_t = [fine_imp_dic[k] for k in dual_ranking_rationale['ner'][i][1]]\n",
    "    dual_ranking_rationale['lexicality'].append([fine_imp_q, fine_imp_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37668f64",
   "metadata": {},
   "source": [
    "### Rationale （Dual ranking + Bi-criteria Denoising)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_sen(sen):\n",
    "    refine_sen = ''.join([i for i in list(sen)])\n",
    "    return refine_sen\n",
    "\n",
    "def refine_sen2(sen):\n",
    "    refine_sen = ''.join([i for i in list(sen) if i not in RM_TOKENS])\n",
    "    return refine_sen\n",
    "\n",
    "def dual_ranking_bi_criteria_denoising(dual_ranking_rationale, rationale_base_pred, rationale_method = 'dual_ranking', denoising = True, denoising_k =3):\n",
    "    ''' \n",
    "    using dual ranking and Bi-criteria denoising algorithm\n",
    "    \n",
    "    input: \n",
    "        dual_ranking_rationale: pd.DataFrame that needs denoise, containing columns \n",
    "            ['id','query','title','piece','rationale', 'lac', 'lexicality','laclime_rank', 'rationale_score'(lime score),'shap_score','ig_score']\n",
    "        rationale_base_pred:  model prediction, list of 0 and 1\n",
    "        rationale_method: string, method of rationale ranking. \n",
    "            Options: {'dual_ranking', 'lexicality_only', 'lac_only', 'positive_lime', 'proportional_lime', 'positive_shap','proportional_shap', 'positive_ig','proportional_ig'}\n",
    "        denoising: bool, whether to use denoising algorithm\n",
    "        denoising_k: int, denoising parameter in Bi-criteria denoising algorithm\n",
    "    output: \n",
    "        final_output: pd.DataFrame with columns ['id', 'label', 'rationale']\n",
    "    '''\n",
    "    dual_ranking_rationale['label'] =  list(rationale_base_pred)\n",
    "    final_output = np.load(LIME_SCORE_PATH, allow_pickle=True).item()\n",
    "\n",
    "    refined_sen_Q = {}\n",
    "    for i in range(len(dual_ranking_rationale['id'])):\n",
    "        refined_sen = refine_sen(dual_ranking_rationale['query'][i])\n",
    "        if refined_sen not in refined_sen_Q:\n",
    "            refined_sen_Q[refined_sen] = 1\n",
    "        else:\n",
    "            refined_sen_Q[refined_sen] += 1\n",
    "        refined_sen = refine_sen(dual_ranking_rationale['title'][i])\n",
    "        if refined_sen not in refined_sen_Q:\n",
    "            refined_sen_Q[refined_sen] = 1\n",
    "        else:\n",
    "            refined_sen_Q[refined_sen] += 1\n",
    "\n",
    "    refined_sen_Q2 = {}\n",
    "    for i in range(len(dual_ranking_rationale['id'])):\n",
    "        refined_sen = refine_sen2(dual_ranking_rationale['query'][i])\n",
    "        if refined_sen not in refined_sen_Q2:\n",
    "            refined_sen_Q2[refined_sen] = 1\n",
    "        else:\n",
    "            refined_sen_Q2[refined_sen] += 1\n",
    "            \n",
    "        refined_sen = refine_sen2(dual_ranking_rationale['title'][i])\n",
    "        if refined_sen not in refined_sen_Q2:\n",
    "            refined_sen_Q2[refined_sen] = 1\n",
    "        else:\n",
    "            refined_sen_Q2[refined_sen] += 1\n",
    "\n",
    "    for i in range(len(dual_ranking_rationale['rationale'])): \n",
    "        text_q_seg = LCQMC_testB[LCQMC_testB['id'] == dual_ranking_rationale['id'][i]]['text_q_seg'].item()\n",
    "        text_t_seg = LCQMC_testB[LCQMC_testB['id'] == dual_ranking_rationale['id'][i]]['text_t_seg'].item()\n",
    "        common_seg = set(text_q_seg).intersection(set(text_t_seg))\n",
    "\n",
    "        text_q  = LCQMC_testB[LCQMC_testB['id'] == dual_ranking_rationale['id'][i]]['query'].item()\n",
    "        text_t  = LCQMC_testB[LCQMC_testB['id'] == dual_ranking_rationale['id'][i]]['title'].item()\n",
    "        if refined_sen_Q[refine_sen(text_q)] > refined_sen_Q[refine_sen(text_t)]:\n",
    "            criteria  = 'ODB_q'\n",
    "        elif refined_sen_Q[refine_sen(text_q)] < refined_sen_Q[refine_sen(text_t)]:\n",
    "            criteria  = 'ODB_t'\n",
    "        else:\n",
    "            if refined_sen_Q2[refine_sen2(text_q)] > refined_sen_Q2[refine_sen2(text_t)]:\n",
    "                criteria = 'RDB_q'\n",
    "            elif refined_sen_Q2[refine_sen2(text_q)] < refined_sen_Q2[refine_sen2(text_t)]:\n",
    "                criteria = 'RDB_t'\n",
    "            else:\n",
    "                criteria = 'NONE'\n",
    "        \n",
    "        \n",
    "        q_seg_from_piece = list(''.join([p for p in dual_ranking_rationale['piece'][i][0] if p not in STOPWORDS+P_LIST]))\n",
    "        remove_q_seg_from_piece = list(''.join([p for p in dual_ranking_rationale['piece'][i][0] if p in STOPWORDS+P_LIST]))\n",
    "        t_seg_from_piece = list(''.join([p for p in dual_ranking_rationale['piece'][i][1] if p not in STOPWORDS+P_LIST]))\n",
    "        remove_t_seg_from_piece = list(''.join([p for p in dual_ranking_rationale['piece'][i][1] if p in STOPWORDS+P_LIST]))\n",
    "        critical_uncommon_seg = set(q_seg_from_piece).union(set(t_seg_from_piece)) - set(q_seg_from_piece).intersection(set(t_seg_from_piece)) - set(remove_q_seg_from_piece).union(set(remove_t_seg_from_piece))-set(STOPWORDS)\n",
    "        \n",
    "        \n",
    "        if dual_ranking_rationale['label'][i] == 1:\n",
    "            if rationale_method == 'positive_lime':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['rationale_score'][i][0][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['rationale_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['rationale_score'][i][1][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['rationale_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "            \n",
    "            elif rationale_method == 'proportional_lime':\n",
    "                rationale = [k for k in range(len(text_q_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['rationale_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_q_seg)*0.705)]\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['rationale_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "\n",
    "            elif rationale_method ==  'positive_shap':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['shap_score'][i][0][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['shap_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['shap_score'][i][1][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['shap_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "            \n",
    "            elif rationale_method ==  'proportional_shap':\n",
    "                rationale = [k for k in range(len(text_q_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['shap_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['shap_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "\n",
    "\n",
    "            elif rationale_method == 'positive_ig':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['ig_score'][i][0][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['ig_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['ig_score'][i][1][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['ig_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "            elif rationale_method == 'proportional_ig':\n",
    "                rationale = [k for k in range(len(text_q_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['ig_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['ig_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "\n",
    "\n",
    "            elif rationale_method == 'lexicality_only':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['lexicality'][i][0][k]>1]\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][0][k] for k in rationale]\n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['lexicality'][i][1][k]>1]\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "            elif rationale_method == 'lac_only':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['lac'][i][0][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][0][k] for k in rationale]\n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['lac'][i][1][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "            elif rationale_method == 'dual_ranking':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['lac'][i][0][k]>0 and text_q_seg[k] not in P_LIST  and (text_q_seg[k] in common_seg or dual_ranking_rationale['lac'][i][0][k]>=2)]\n",
    "                max_LAC_imp = max([dual_ranking_rationale['lexicality'][i][0][k] for k in rationale])\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][0][k]+ min(max_LAC_imp-dual_ranking_rationale['lexicality'][i][0][k], dual_ranking_rationale['lac'][i][0][k]/100) for k in rationale]\n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['lac'][i][1][k] >0 and text_t_seg[k] not in P_LIST and (text_t_seg[k] in common_seg or dual_ranking_rationale['lac'][i][1][k]>=2)]\n",
    "                max_LAC_imp = max([dual_ranking_rationale['lexicality'][i][1][k] for k in rationale])\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][1][k]+ min(max_LAC_imp-dual_ranking_rationale['lexicality'][i][1][k], dual_ranking_rationale['lac'][i][1][k]/100) for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                            \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            #denoising for alignment（label 1)\n",
    "            if denoising and (criteria == 'ODB_q' or criteria == 'RDB_q'):\n",
    "                new_rationale = []\n",
    "                for qk in final_output['rationale'][i][0]:\n",
    "                    for tk in final_output['rationale'][i][1]:\n",
    "                        if text_t_seg[tk] == text_q_seg[qk]:\n",
    "                            new_rationale.append(tk)\n",
    "                            final_output['rationale'][i][1].remove(tk)\n",
    "                            break\n",
    "                final_output['rationale'][i][1] = new_rationale\n",
    "            \n",
    "            if denoising and (criteria == 'ODB_t' or criteria == 'RDB_t'):\n",
    "                new_rationale = []\n",
    "                for tk in final_output['rationale'][i][1]:\n",
    "                    for qk in final_output['rationale'][i][0]:\n",
    "                        if text_q_seg[qk] == text_t_seg[tk]:\n",
    "                            new_rationale.append(qk)\n",
    "                            final_output['rationale'][i][0].remove(qk)\n",
    "                            break\n",
    "                final_output['rationale'][i][0] = new_rationale\n",
    "\n",
    "\n",
    "        if dual_ranking_rationale['label'][i] == 0:\n",
    "            if rationale_method == 'positive_lime':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['rationale_score'][i][0][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['rationale_score'][i][0][k] for k in rationale]\n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['rationale_score'][i][1][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['rationale_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "            elif rationale_method == 'proportional_lime':\n",
    "                rationale = [k for k in range(len(text_q_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['rationale_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_q_seg)*0.705)]\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['rationale_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "\n",
    "            elif rationale_method == 'positive_shap':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['shap_score'][i][0][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['shap_score'][i][0][k] for k in rationale]\n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['shap_score'][i][1][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['shap_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "            elif rationale_method ==  'proportional_shap':\n",
    "                rationale = [k for k in range(len(text_q_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['shap_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['shap_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "\n",
    "            elif rationale_method == 'positive_ig':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['ig_score'][i][0][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['ig_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['ig_score'][i][1][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['ig_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "            elif rationale_method == 'proportional_ig':\n",
    "                rationale = [k for k in range(len(text_q_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['ig_score'][i][0][k] for k in rationale]                \n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg))]\n",
    "                LAC_imp = [dual_ranking_rationale['ig_score'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index'][:math.ceil(len(text_t_seg)*0.705)]\n",
    "\n",
    "            elif rationale_method == 'lexicality_only':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['lexicality'][i][0][k]>1]\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][0][k] for k in rationale]\n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['lexicality'][i][1][k]>1]\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "           \n",
    "            elif rationale_method == 'lac_only':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['lac'][i][0][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][0][k] for k in rationale]\n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['lac'][i][1][k]>0]\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "\n",
    "            elif rationale_method == 'dual_ranking':\n",
    "                rationale = [k for k in range(len(text_q_seg)) if dual_ranking_rationale['lac'][i][0][k]>0 and text_q_seg[k] not in P_LIST]\n",
    "                max_LAC_imp = max([dual_ranking_rationale['lexicality'][i][0][k] for k in rationale])\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][0][k]+min(max_LAC_imp-dual_ranking_rationale['lexicality'][i][0][k], dual_ranking_rationale['laclime_rank'][i][0][k]/100 ) if text_q_seg[k] in critical_uncommon_seg else dual_ranking_rationale['lexicality'][i][0][k] for k in rationale]\n",
    "                final_output['rationale'][i][0]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "                \n",
    "                rationale = [k for k in range(len(text_t_seg)) if dual_ranking_rationale['lac'][i][1][k] >0 and text_t_seg[k] not in P_LIST]\n",
    "                max_LAC_imp = max([dual_ranking_rationale['lexicality'][i][1][k] for k in rationale])\n",
    "                LAC_imp = [dual_ranking_rationale['lexicality'][i][1][k]+min(max_LAC_imp-dual_ranking_rationale['lexicality'][i][1][k], dual_ranking_rationale['laclime_rank'][i][1][k]/100) if text_t_seg[k] in critical_uncommon_seg else dual_ranking_rationale['lexicality'][i][1][k] for k in rationale]\n",
    "                final_output['rationale'][i][1]  = reorder(LAC_imp, rationale, mode='sequence')['sorted_token_index']\n",
    "           \n",
    "    \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            #denoising for alignment (label 0)\n",
    "            if denoising and (criteria == 'ODB_q' or criteria == 'RDB_q'):\n",
    "                new_rationale = []\n",
    "                uncommon_flag = 0 \n",
    "                for qk in final_output['rationale'][i][0]:\n",
    "                    if len(final_output['rationale'][i][1])==0:\n",
    "                        break                \n",
    "                    first_t = final_output['rationale'][i][1][0]\n",
    "                    \n",
    "                    if text_t_seg[first_t]== text_q_seg[qk]:\n",
    "                        new_rationale.append(first_t)\n",
    "                        final_output['rationale'][i][1].remove(first_t)\n",
    "                        if len(final_output['rationale'][i][1])>0:\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "                            \n",
    "                    else:                    \n",
    "                        if text_q_seg[qk] not in text_t_seg:\n",
    "                            if uncommon_flag < denoising_k:\n",
    "                                uncommon_flag +=1 \n",
    "                                uncommon_t_1st = final_output['rationale'][i][1][0] \n",
    "                                new_rationale.append(uncommon_t_1st)\n",
    "                                final_output['rationale'][i][1].remove(uncommon_t_1st)\n",
    "                                continue\n",
    "                            else:\n",
    "                                break\n",
    "                        else:\n",
    "                            for tk in final_output['rationale'][i][1]:\n",
    "                                if text_t_seg[tk] == text_q_seg[qk]:\n",
    "                                    new_rationale.append(tk)\n",
    "                                    final_output['rationale'][i][1].remove(tk)\n",
    "                                    break\n",
    "                new_rationale += final_output['rationale'][i][1]\n",
    "                final_output['rationale'][i][1] = new_rationale\n",
    "                \n",
    "                \n",
    "            if denoising and (criteria == 'ODB_t' or criteria == 'RDB_t'):\n",
    "                new_rationale = []\n",
    "                uncommon_flag = 0 \n",
    "                for tk in final_output['rationale'][i][1]:\n",
    "                    if len(final_output['rationale'][i][0])==0:\n",
    "                        break\n",
    "                        \n",
    "                    first_q = final_output['rationale'][i][0][0]\n",
    "                    \n",
    "                    if text_q_seg[first_q]== text_t_seg[tk]:\n",
    "                        new_rationale.append(first_q)\n",
    "                        final_output['rationale'][i][0].remove(first_q)\n",
    "                        if len(final_output['rationale'][i][0])>0:\n",
    "                            continue \n",
    "                        else:\n",
    "                            break\n",
    "                            \n",
    "                    else:                    \n",
    "                        if text_t_seg[tk] not in text_q_seg:\n",
    "                            if uncommon_flag < denoising_k:\n",
    "                                uncommon_flag +=1 \n",
    "                                uncommon_q_1st = final_output['rationale'][i][0][0] \n",
    "                                new_rationale.append(uncommon_q_1st)\n",
    "                                final_output['rationale'][i][0].remove(uncommon_q_1st)\n",
    "                                continue\n",
    "                            else:\n",
    "                                break\n",
    "                        else:\n",
    "                            for qk in final_output['rationale'][i][0]:\n",
    "                                if text_q_seg[qk] == text_t_seg[tk]:\n",
    "                                    new_rationale.append(qk)\n",
    "                                    final_output['rationale'][i][0].remove(qk)\n",
    "                                    break\n",
    "                new_rationale += final_output['rationale'][i][0]\n",
    "                final_output['rationale'][i][0] = new_rationale\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5246f63",
   "metadata": {},
   "source": [
    "### Write outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method = 'dual_ranking', denoising = True)\n",
    "out_file = open('./rationale_results/sim_rationale_dual_ranking.txt', 'w')\n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9bb10",
   "metadata": {},
   "source": [
    "### Some baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No denoising \n",
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method = 'dual_ranking', denoising = False)\n",
    "out_file = open('./rationale_results/sim_rationale_wo_denoising.txt', 'w')\n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdaa127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicality ranking only \n",
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method= 'lexicality_only',  denoising = False)\n",
    "out_file = open('./rationale_results/sim_rationale_lexicality.txt', 'w')\n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60305558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime positive\n",
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method = 'positive_lime', denoising = False)\n",
    "out_file = open('./rationale_results/sim_rationale_lime.txt', 'w')\n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportional lime\n",
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method = 'proportional_lime', denoising = False)\n",
    "out_file = open('./rationale_results/sim_rationale_proportional_lime.txt', 'w')  \n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap positive \n",
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method = 'positive_shap', denoising = False)\n",
    "out_file = open('./rationale_results/sim_rationale_shap.txt', 'w')\n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32846b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportional shap  \n",
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method = 'proportional_shap', denoising = False)\n",
    "out_file = open('./rationale_results/sim_rationale_proportional_shap.txt', 'w')\n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ig positive\n",
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method = 'positive_ig', denoising = False)\n",
    "out_file = open('./rationale_results/sim_rationale_ig.txt', 'w')\n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed532ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportional ig\n",
    "final_output = dual_ranking_bi_criteria_denoising(dual_ranking_rationale, final_prediction_macbert_EF, rationale_method = 'proportional_ig', denoising = False)\n",
    "out_file = open('./rationale_results/sim_rationale_proportional_ig.txt', 'w')\n",
    "for i in range(len(final_output['id'])):\n",
    "    out_file.write(str(final_output['id'][i]) + '\\t'+ str(final_output['label'][i]) + '\\t' +\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][0]]) +'\\t'+\n",
    "                   ','.join([str(i) for i in final_output['rationale'][i][1]]) +'\\n')\n",
    "out_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
